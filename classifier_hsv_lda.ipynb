{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048c5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from skimage import exposure\n",
    "from skimage import color\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.morphology import binary_erosion\n",
    "from skimage.measure import regionprops, find_contours\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from utils import *\n",
    "from feature_util import *\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image paths and labels from the DataFrame\n",
    "df = pd.read_csv('Dataset/cards.csv')\n",
    "\n",
    "# For column names that contain space, replace the space with an underscore\n",
    "df.columns = [c.replace(' ', '_') for c in df.columns]\n",
    "\n",
    "# Add suits column\n",
    "df['suit'] = df['labels'].str.split().str[-1]\n",
    "\n",
    "# Remove rows with jokers\n",
    "df = df[~df['suit'].str.contains('joker', case=False)]\n",
    "\n",
    "# Remove unwanted columns\n",
    "df = df.drop(columns = ['data_set'])\n",
    "df = df.drop(columns = ['class_index'])\n",
    "df = df.drop(columns = ['labels'])\n",
    "df = df.drop(columns = ['card_type'])\n",
    "\n",
    "# Add folder name to the filepath\n",
    "df['filepaths'] = df['filepaths'].apply(lambda x: 'Dataset/' + x)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fabeb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataframe into train and test sets, stratified by the 'suit' column\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['suit'], random_state=42)\n",
    "\n",
    "# Split train set into train and validation sets, stratified by the 'suit' column\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['suit'], random_state=42)\n",
    "\n",
    "# Check class distribution in the train, validation, and test sets\n",
    "train_class_distribution = train_df['suit'].value_counts(normalize=True)\n",
    "val_class_distribution = val_df['suit'].value_counts(normalize=True)\n",
    "test_class_distribution = test_df['suit'].value_counts(normalize=True)\n",
    "\n",
    "print(\"Train Set - Class Distribution:\")\n",
    "print(train_class_distribution)\n",
    "\n",
    "print(\"Validation Set - Class Distribution:\")\n",
    "print(val_class_distribution)\n",
    "\n",
    "print(\"Test Set - Class Distribution:\")\n",
    "print(test_class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b360a9b",
   "metadata": {},
   "source": [
    "## Augment Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd1b9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data generator class\n",
    "\n",
    "train_data_generator = ImageDataGenerator(\n",
    "#    rescale = 1/255.0,   #Want 0 to 255 values for the color histogram\n",
    "#     rotation_range= 45,\n",
    "#     zoom_range= 0.2,\n",
    "#     width_shift_range = 0.2,\n",
    "#     height_shift_range = 0.2,\n",
    "#     shear_range= 0.2,\n",
    "#     horizontal_flip=True,\n",
    "#     vertical_flip=True\n",
    " )\n",
    "\n",
    "test_data_generator = ImageDataGenerator() #rescale = 1/255.0) #want 0 to 255 values for the color histogram\n",
    "\n",
    "# Create data generators for train, validation, and test\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = train_data_generator.flow_from_dataframe(\n",
    "    dataframe = train_df,\n",
    "    x_col = 'filepaths',\n",
    "    y_col = 'suit',\n",
    "    target_size = (224,224),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "valid_generator = test_data_generator.flow_from_dataframe( #change this to use test_data_generator since don't want to augment the valid images\n",
    "    dataframe = val_df,\n",
    "    x_col = 'filepaths',\n",
    "    y_col = 'suit',\n",
    "    target_size = (224,224),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = test_data_generator.flow_from_dataframe( #change this to use test_data_generator since don't want to augment the valid images\n",
    "    dataframe = test_df,\n",
    "    x_col = 'filepaths',\n",
    "    y_col = 'suit',\n",
    "    target_size = (224,224),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'categorical',\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e5af2a",
   "metadata": {},
   "source": [
    "### Hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43437342",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hist_hue(image):\n",
    "    \n",
    "    # Convert the image from RGB to HSV\n",
    "    image_hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    \n",
    "    # Calculate histograms for each channel\n",
    "    hist_hue = cv2.calcHist([image_hsv], [0], None , [255], [0, 256]).astype(np.uint8)\n",
    "\n",
    "    #normalize \n",
    "    hist_hue_normalized = (hist_hue - np.min(hist_hue)) / (np.max(hist_hue)  - np.min(hist_hue) )\n",
    "    \n",
    "    # Scale the normalized histogram to have values between 0 and 255\n",
    "    hist_hue_scaled = (hist_hue_normalized * 255).astype(np.uint8)\n",
    "    \n",
    "    return hist_hue_scaled\n",
    "\n",
    "# Function to compute hsv features for a batch of images\n",
    "def extract_hue_features_from_generator(data_generator):\n",
    "    hsv_features = []\n",
    "    labels = []\n",
    "    num_batches = len(data_generator)\n",
    "    \n",
    "    for _ in range(num_batches):\n",
    "        batch_images, class_labels = data_generator.next()  # Get the next batch of augmented images (ignoring the labels)\n",
    "        \n",
    "        # Compute hue features for each image in the batch\n",
    "        for image, label in zip(batch_images,class_labels):\n",
    "            hue_feature = hist_hue(image).flatten()  \n",
    "            hsv_features.append(hue_feature)\n",
    "            label = list(label)\n",
    "            labels.append(label)\n",
    "            \n",
    "    return np.array(hsv_features),labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824f9848",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_hue_features, train_labels = extract_hue_features_from_generator(train_generator)\n",
    "valid_hue_features, valid_labels = extract_hue_features_from_generator(valid_generator)\n",
    "test_hue_features, test_labels = extract_hue_features_from_generator(test_generator)\n",
    "\n",
    "train_labels = np.array(train_labels)\n",
    "valid_labels = np.array(valid_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfd02ae",
   "metadata": {},
   "source": [
    "### LDA Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a012025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the LDA model\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(train_hue_features, train_generator.labels)\n",
    "\n",
    "# Transform the validation features to the LDA space\n",
    "valid_features_lda = lda_model.transform(valid_hue_features)\n",
    "\n",
    "# Predict on the validation data\n",
    "valid_predictions = lda_model.predict(valid_hue_features)\n",
    "\n",
    "\n",
    "# Calculate the accuracy of the LDA model\n",
    "accuracy = accuracy_score(valid_generator.labels,valid_predictions)\n",
    "print(\"Validation Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381d9a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class names from the generator (assuming it has the 'class_indices' attribute)\n",
    "class_indices = train_generator.class_indices\n",
    "class_names = list(class_indices.keys())\n",
    "\n",
    "# Calculate classification report for validation set\n",
    "valid_report = classification_report(valid_generator.labels, valid_predictions)\n",
    "print(\"Validation Classification Report:\\n\", valid_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195adf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(valid_generator.labels,valid_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion,display_labels = class_names)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9216c4cb",
   "metadata": {},
   "source": [
    "### Grid search LDA: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6654341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'solver': ['svd', 'lsqr', 'eigen'],  # Different solver options for LDA\n",
    "    'shrinkage': [None, 'auto', 0.1, 0.5],  # Shrinkage parameter (None means no shrinkage)\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV with the LDA model and parameter grid\n",
    "grid_search = GridSearchCV(lda_model, param_grid, cv=5)\n",
    "\n",
    "# Fit the GridSearchCV on the training data\n",
    "grid_search.fit(train_hue_features, train_generator.labels)\n",
    "\n",
    "# Get the best LDA model from the grid search\n",
    "best_lda_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the validation data using the best model\n",
    "valid_predictions = best_lda_model.predict(valid_hue_features)\n",
    "\n",
    "# Calculate the accuracy of the best LDA model\n",
    "accuracy = accuracy_score(valid_generator.labels, valid_predictions)\n",
    "print(\"Validation Accuracy with LDA:\", accuracy)\n",
    "\n",
    "# Print the best hyperparameters found by GridSearchCV\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c73796",
   "metadata": {},
   "source": [
    "### hue and hog combined feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74fcfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_hog_hue_features_from_generator(data_generator):\n",
    "    hog_features = []\n",
    "    hsv_features = []\n",
    "    labels = []\n",
    "    num_batches = len(data_generator)\n",
    "    \n",
    "    for _ in range(num_batches):\n",
    "        batch_images, class_labels = data_generator.next()  # Get the next batch of augmented images (ignoring the labels)\n",
    "        \n",
    "        # Compute hue features for each image in the batch\n",
    "        for image, label in zip(batch_images,class_labels):\n",
    "           \n",
    "            hue_feature = hist_hue(image).flatten()  \n",
    "            hsv_features.append(hue_feature)\n",
    "            label = list(label)\n",
    "            labels.append(label)\n",
    "            gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) \n",
    "            gray_image = gray_image.astype(np.uint8)\n",
    "            hog_feature = hog(gray_image, orientations = 8, pixels_per_cell=(16, 16), cells_per_block=(4, 4))\n",
    "            hog_features.append(hog_feature)\n",
    "            \n",
    "    hog_hue_feature = np.hstack((hog_features, hsv_features)) #hog_features \n",
    "            \n",
    "    return np.array(hog_hue_feature), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c644048",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hog_hue_features, train_labels = extract_hog_hue_features_from_generator(train_generator)\n",
    "valid_hog_hue_features, valid_labels = extract_hog_hue_features_from_generator(valid_generator)\n",
    "test_hog_hue_features, test_labels = extract_hog_hue_features_from_generator(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e641ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hog_hue_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a55c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel = 'linear',C=0.1)\n",
    "clf.fit(train_hog_hue_features,train_generator.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c89be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the validation data\n",
    "valid_predictions = clf.predict(valid_hog_hue_features)\n",
    "\n",
    "# Calculate the accuracy of the LDA model\n",
    "accuracy = accuracy_score(valid_generator.labels,valid_predictions)\n",
    "print(\"Validation Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e1eade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on the HOG features.\n",
    "pca = PCA(n_components = 100)\n",
    "train_hog_hue_features_pca = pca.fit_transform(train_hog_hue_features)\n",
    "valid_hog_hue_features_pca = pca.fit_transform(valid_hog_hue_features)\n",
    "\n",
    "# Cumulative explained variance.\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Determine the number of principal components to plot with increments of 100.\n",
    "num_components = 100 #hog_hue_features_pca.shape[1]\n",
    "components_range = np.arange(1, num_components + 1, 10)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(components_range, cumulative_explained_variance[components_range - 1], marker='o')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA - Cumulative Explained Variance')\n",
    "plt.xticks(components_range, fontsize=6)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96b417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the LDA model\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(train_hog_hue_features_pca, train_generator.labels)\n",
    "\n",
    "# Transform the validation features to the LDA space\n",
    "#valid_features_lda = lda_model.transform(valid_hog_hue_features_pca)\n",
    "\n",
    "# Predict on the validation data\n",
    "valid_predictions = lda_model.predict(valid_hog_hue_features_pca)\n",
    "\n",
    "\n",
    "# Calculate the accuracy of the LDA model\n",
    "accuracy = accuracy_score(valid_generator.labels,valid_predictions)\n",
    "print(\"Validation Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
