{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e372cf81",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae4f2ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.10.1)\n",
      "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.8.8)\n",
      "Requirement already satisfied: imageio>=2.27 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.31.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2023.7.10)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.4.1)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (0.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikeras in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
      "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras) (23.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.22.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-29 10:57:41.020138: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-29 10:57:41.043357: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib scikit-image\n",
    "!pip install scikeras\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from skimage import filters\n",
    "from skimage import exposure\n",
    "from skimage import color\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import hog\n",
    "from skimage.filters import gaussian\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from scipy.stats import loguniform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dda746",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "422ab76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepaths</th>\n",
       "      <th>suit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dataset/train/ace of clubs/001.jpg</td>\n",
       "      <td>clubs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dataset/train/ace of clubs/002.jpg</td>\n",
       "      <td>clubs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dataset/train/ace of clubs/003.jpg</td>\n",
       "      <td>clubs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dataset/train/ace of clubs/004.jpg</td>\n",
       "      <td>clubs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dataset/train/ace of clubs/005.jpg</td>\n",
       "      <td>clubs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            filepaths   suit\n",
       "0  Dataset/train/ace of clubs/001.jpg  clubs\n",
       "1  Dataset/train/ace of clubs/002.jpg  clubs\n",
       "2  Dataset/train/ace of clubs/003.jpg  clubs\n",
       "3  Dataset/train/ace of clubs/004.jpg  clubs\n",
       "4  Dataset/train/ace of clubs/005.jpg  clubs"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the image paths and labels from the DataFrame\n",
    "df = pd.read_csv('Dataset/cards.csv')\n",
    "\n",
    "# For column names that contain space, replace the space with an underscore\n",
    "df.columns = [c.replace(' ', '_') for c in df.columns]\n",
    "\n",
    "# Add suits column\n",
    "df['suit'] = df['labels'].str.split().str[-1]\n",
    "\n",
    "# Remove rows with jokers\n",
    "df = df[~df['suit'].str.contains('joker', case=False)]\n",
    "\n",
    "# Remove unwanted columns\n",
    "df = df.drop(columns = ['data_set'])\n",
    "df = df.drop(columns = ['class_index'])\n",
    "df = df.drop(columns = ['labels'])\n",
    "df = df.drop(columns = ['card_type'])\n",
    "\n",
    "# Add folder name to the filepath\n",
    "df['filepaths'] = df['filepaths'].apply(lambda x: 'Dataset/' + x)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbf472b",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56cc11c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataframe into train and test sets, stratified by the 'suit' column\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['suit'], random_state=42)\n",
    "\n",
    "# Split train set into train and validation sets, stratified by the 'suit' column\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['suit'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01092bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set - Class Distribution:\n",
      "spades      0.269171\n",
      "hearts      0.246010\n",
      "diamonds    0.243675\n",
      "clubs       0.241144\n",
      "Name: suit, dtype: float64\n",
      "Validation Set - Class Distribution:\n",
      "spades      0.269261\n",
      "hearts      0.245914\n",
      "diamonds    0.243580\n",
      "clubs       0.241245\n",
      "Name: suit, dtype: float64\n",
      "Test Set - Class Distribution:\n",
      "spades      0.268991\n",
      "hearts      0.245953\n",
      "diamonds    0.244085\n",
      "clubs       0.240971\n",
      "Name: suit, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution in the train, validation, and test sets\n",
    "train_class_distribution = train_df['suit'].value_counts(normalize=True)\n",
    "val_class_distribution = val_df['suit'].value_counts(normalize=True)\n",
    "test_class_distribution = test_df['suit'].value_counts(normalize=True)\n",
    "\n",
    "print(\"Train Set - Class Distribution:\")\n",
    "print(train_class_distribution)\n",
    "\n",
    "print(\"Validation Set - Class Distribution:\")\n",
    "print(val_class_distribution)\n",
    "\n",
    "print(\"Test Set - Class Distribution:\")\n",
    "print(test_class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0751c882",
   "metadata": {},
   "source": [
    "## Augment Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1255c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize data generator class\n",
    "# train_data_generator = ImageDataGenerator(\n",
    "#     rescale = 1/255.0,\n",
    "#     rotation_range= 45,\n",
    "#     zoom_range= 0.2,\n",
    "#     width_shift_range = 0.2,\n",
    "#     height_shift_range = 0.2,\n",
    "#     shear_range= 0.2,\n",
    "#     horizontal_flip=True,\n",
    "#     vertical_flip=True\n",
    "# )\n",
    "\n",
    "# test_data_generator = ImageDataGenerator(rescale = 1/255.0)\n",
    "\n",
    "# # Create data generators for train, validation, and test\n",
    "# batch_size = 32\n",
    "\n",
    "# train_generator = train_data_generator.flow_from_dataframe(\n",
    "#     dataframe = train_df,\n",
    "#     x_col = 'filepaths',\n",
    "#     y_col = 'suit',\n",
    "#     target_size = (224,224),\n",
    "#     batch_size = batch_size,\n",
    "#     class_mode = 'categorical'\n",
    "# )\n",
    "\n",
    "# valid_generator = train_data_generator.flow_from_dataframe(\n",
    "#     dataframe = val_df,\n",
    "#     x_col = 'filepaths',\n",
    "#     y_col = 'suit',\n",
    "#     target_size = (224,224),\n",
    "#     batch_size = batch_size,\n",
    "#     class_mode = 'categorical'\n",
    "# )\n",
    "\n",
    "\n",
    "# test_generator = test_data_generator.flow_from_dataframe(\n",
    "#     dataframe = test_df,\n",
    "#     x_col = 'filepaths',\n",
    "#     y_col = 'suit',\n",
    "#     target_size = (224,224),\n",
    "#     batch_size = batch_size,\n",
    "#     class_mode = 'categorical',\n",
    "#     shuffle = False,\n",
    "# )\n",
    "\n",
    "# del train_df\n",
    "# del test_df\n",
    "# del val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58840230",
   "metadata": {},
   "source": [
    "## Extract HOG Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9a5c507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    return image_gray\n",
    "\n",
    "\n",
    "def extract_hog_features(image_paths, labels):\n",
    "    featurelist = []\n",
    "    hog_example = None\n",
    "    label_list = []\n",
    "\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        img = load_img(image_path)\n",
    "        image_array = img_to_array(img)\n",
    "        gray_image = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)\n",
    "        fd, hog_image = hog(gray_image, \n",
    "                            pixels_per_cell=(8, 8),\n",
    "                            cells_per_block=(2, 2), \n",
    "                            orientations=4, \n",
    "                            visualize=True)\n",
    "\n",
    "        if i == 0:\n",
    "            hog_example = hog_image\n",
    "\n",
    "        featurelist.append(fd[np.newaxis, :])\n",
    "        label_list.append(labels[i])\n",
    "\n",
    "    features = np.vstack(featurelist)\n",
    "    labels = np.array(label_list)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "# def extract_hog_features(images, labels):\n",
    "#     featurelist = []\n",
    "#     hog_example = None\n",
    "#     label_list = []\n",
    "    \n",
    "#     for i in range(images.shape[0]):\n",
    "#         gray_image = rgb2gray(images[i, :, :])\n",
    "#         fd, hog_image = hog(gray_image, \n",
    "#                             pixels_per_cell=(8, 8),\n",
    "#                             cells_per_block=(2, 2), \n",
    "#                             orientations=4, \n",
    "#                             visualize=True)\n",
    "        \n",
    "#         if i == 0:\n",
    "#             hog_example = hog_image\n",
    "        \n",
    "#         featurelist.append(fd[np.newaxis, :])\n",
    "#         label_list.append(labels[i])\n",
    "        \n",
    "#     features = np.vstack(featurelist)\n",
    "#     labels = np.array(label_list)\n",
    "    \n",
    "#     return features, labels\n",
    "\n",
    "# def extract_hog_features_batch(images, labels):\n",
    "#     feature_list = []\n",
    "#     label_list = []\n",
    "    \n",
    "#     for i in range(images.shape[0]):\n",
    "#         gray_image = rgb2gray(images[i, :, :, :])  # Convert RGB to grayscale\n",
    "#         fd, _ = hog(gray_image,\n",
    "#                     pixels_per_cell=(8, 8),\n",
    "#                     cells_per_block=(2, 2),\n",
    "#                     orientations=4,\n",
    "#                     visualize=True)\n",
    "        \n",
    "#         feature_list.append(fd)\n",
    "#         label_list.append(labels[i])\n",
    "        \n",
    "#     return np.array(feature_list), np.array(label_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "664b81be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a portion of the train set\n",
    "train_fraction = 0.5  # Change this fraction to the desired portion\n",
    "\n",
    "train_df_subset, _ = train_test_split(train_df, train_size=train_fraction, stratify=train_df['suit'], random_state=42)\n",
    "train_features, train_labels = extract_hog_features(train_df_subset['filepaths'].values, train_df_subset['suit'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "774c1465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a portion of the validation set\n",
    "val_fraction = 0.2  # Change this fraction to the desired portion\n",
    "\n",
    "val_df_subset, _ = train_test_split(val_df, test_size=val_fraction, stratify=val_df['suit'], random_state=42)\n",
    "val_features, val_labels = extract_hog_features(val_df_subset['filepaths'].values, val_df_subset['suit'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa3bd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features, test_labels = extract_hog_features(test_df_subset['filepaths'].values, test_df_subset['suit'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e7b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract HOG features for train, validation, and test sets\n",
    "# train_features, train_labels = extract_hog_features(train_df['filepaths'].values, train_df['suit'].values)\n",
    "# val_features, val_labels = extract_hog_features(val_df['filepaths'].values, val_df['suit'].values)\n",
    "# test_features, test_labels = extract_hog_features(test_df['filepaths'].values, test_df['suit'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db44eade",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b681154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6352140077821011\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, validation_accuracy)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Optionally, evaluate the SVM on the test set\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m svm_classifier\u001b[38;5;241m.\u001b[39mscore(\u001b[43mtest_features\u001b[49m, test_labels)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_accuracy)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_features' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize and train the SVM classifier\n",
    "svm_classifier = SVC(kernel='linear', random_state=42)  # You can experiment with different kernels\n",
    "svm_classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Evaluate the SVM on the validation set\n",
    "validation_accuracy = svm_classifier.score(val_features, val_labels)\n",
    "print(\"Validation Accuracy:\", validation_accuracy)\n",
    "\n",
    "# Optionally, evaluate the SVM on the test set\n",
    "test_accuracy = svm_classifier.score(test_features, test_labels)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2d2a44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07492f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the SVM classifier\n",
    "svm_classifier = SVC(kernel='linear', random_state=42)\n",
    "svm_classifier.fit(train_features, np.argmax(train_labels, axis=1))  # use argmax to convert one-hot to categorical labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea13937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract HOG features from the validation set\n",
    "# valid_features, valid_labels = extract_hog_features(valid_generator[0][0], valid_generator[0][1])\n",
    "\n",
    "# Make predictions on the validation set\n",
    "valid_predictions = svm_classifier.predict(valid_features)\n",
    "valid_labels_categorical = np.argmax(valid_labels, axis=1)\n",
    "\n",
    "\n",
    "# Evaluate the SVM classifier on the validation set\n",
    "valid_accuracy = svm_classifier.score(valid_features, np.argmax(valid_labels, axis=1))\n",
    "print(\"Valid accuracy:\", valid_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2278bf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract HOG features from the test set\n",
    "# test_features, test_labels = extract_hog_features(test_generator[0][0], test_generator[0][1])\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = svm_classifier.predict(test_features)\n",
    "test_labels_categorical = np.argmax(test_labels, axis=1)\n",
    "\n",
    "# Evaluate the SVM classifier on the test set\n",
    "test_accuracy = svm_classifier.score(test_features, np.argmax(test_labels, axis=1))\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9ea0dc",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4b67e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the true labels from the test generator and flatten to 1D array\n",
    "# true_labels = np.array(test_generator[0][1].argmax(axis=1)).flatten()\n",
    "\n",
    "# # Convert the predicted labels to numpy array\n",
    "# predicted_labels = test_predictions.flatten()\n",
    "\n",
    "conf_matrix = confusion_matrix(valid_labels_categorical, valid_predictions)\n",
    "\n",
    "# Define the class labels\n",
    "class_labels = ['spades', 'hearts', 'diamonds', 'clubs']\n",
    "\n",
    "# Create the seaborn heatmap\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True, \n",
    "            fmt=\"d\", \n",
    "            cmap=\"Blues\", \n",
    "            xticklabels=class_labels, \n",
    "            yticklabels=class_labels)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1344128",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454bf210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class names from the generator\n",
    "class_names = list(train_generator.class_indices.keys())\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191c61a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate classification report for validation set\n",
    "valid_report = classification_report(valid_labels_categorical, valid_predictions, target_names=class_names)\n",
    "print(\"Validation Classification Report:\\n\", valid_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dc5b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate classification report for test set\n",
    "test_report = classification_report(test_labels_categorical, test_predictions, target_names=class_names)\n",
    "print(\"Test Classification Report:\\n\", test_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a336baa",
   "metadata": {},
   "source": [
    "## PCA Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cb60d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on the HOG features.\n",
    "pca = PCA()\n",
    "hog_features_pca = pca.fit_transform(train_features)\n",
    "\n",
    "# Cumulative explained variance.\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Plot fifty principal components\n",
    "num_components = min(50, len(cumulative_explained_variance))\n",
    "components_range = np.arange(1, num_components + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(components_range, cumulative_explained_variance[components_range - 1], marker='o')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA - Cumulative Explained Variance')\n",
    "plt.xticks(components_range, fontsize=6)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054a2677",
   "metadata": {},
   "source": [
    "## tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317b7e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute tSNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_result = tsne.fit_transform(data_features)\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['red', 'blue', 'green', 'purple']  # Colors for the 4 classes\n",
    "class_labels = ['spades', 'hearts', 'diamonds', 'clubs']\n",
    "\n",
    "for i in range(4):\n",
    "    # Get the indices of data points with the current class label\n",
    "    indices = np.where(data_labels == i)[0]\n",
    "    \n",
    "    # Scatter plot the t-SNE points with the corresponding class label\n",
    "    plt.scatter(tsne_result[indices, 0], tsne_result[indices, 1], c=colors[i], label=class_labels[i])\n",
    "\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "plt.title(\"t-SNE Visualization for 4 Card Suit Classes\")\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cc21cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionality reduction\n",
    "X_pixels_pca, X_hog_pca = get_PCA([pixel_features, hog_features], n_components=2)[-1]\n",
    "X_pixels_tsne, X_hog_tsne = get_tsne([pixel_features, hog_features], n_components=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
